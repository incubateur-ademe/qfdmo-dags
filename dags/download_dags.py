import logging
from datetime import datetime, timedelta
from pathlib import Path

import airflow.configuration as conf
from airflow import DAG
from airflow.models import DagBag
from airflow.operators.python import PythonOperator
from airflow.providers.amazon.aws.hooks.s3 import S3Hook

dags_dirs = ["preprod", "production"]


def download_dags_from_s3():
    s3_hook = S3Hook(aws_conn_id="s3dags")
    bucket_name = "qfdmo-airflow-dags"
    keys = s3_hook.list_keys(bucket_name)
    for key in keys:
        dags_folder = conf.get("core", "dags_folder")
        logging.warning(f"Downloading {key} from S3 to {dags_folder}")
        file_path = Path(dags_folder, key)
        file_path.unlink(missing_ok=True)
        parent_folder = file_path.parent
        parent_folder.mkdir(parents=True, exist_ok=True)
        s3_hook.download_file(
            key,
            bucket_name=bucket_name,
            local_path=parent_folder,
            preserve_file_name=True,
            use_autogenerated_subdir=False,
        )
    for subdir in dags_dirs:
        dag_bag = DagBag(Path(dags_folder, subdir))
        if dag_bag:
            for dag_id, dag in dag_bag.dags.items():
                globals()[subdir + "_" + dag_id] = dag


default_args = {
    "owner": "airflow",
    "depends_on_past": False,
    "start_date": datetime(2022, 1, 1),
    "email_on_failure": False,
    "email_on_retry": False,
    "retries": 1,
    "retry_delay": timedelta(minutes=5),
}

with DAG(
    "download_dags_from_s3",
    default_args=default_args,
    description="DAG to download dags from S3",
    schedule_interval=timedelta(days=1),
    catchup=False,
) as dag:

    download_dags = PythonOperator(
        task_id="download_dags_from_s3", python_callable=download_dags_from_s3, dag=dag
    )

    download_dags
